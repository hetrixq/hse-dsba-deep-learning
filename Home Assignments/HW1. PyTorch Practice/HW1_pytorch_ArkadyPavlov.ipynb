{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcF9My_FhLuk"
   },
   "source": [
    "# **HW 1. PyTorch practice**\n",
    "### Course: Deep Learning (DSBA and ICEF), 2026, HSE\n",
    "### Authors: Alexey Boldyrev, ML Teaching Team\n",
    "\n",
    "Release Date: 23.01.2026\n",
    "\n",
    "Deadline: \\\n",
    "(Soft) 23:59 MSK 01.02.2026 \\\n",
    "(Hard) 18:00 MSK 02.02.2026\n",
    "\n",
    "Authors: Alexey Boldyrev, ML Teaching Team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G64TptyJhLuo"
   },
   "source": [
    "### About the assignment\n",
    "\n",
    "The assignment is in two parts. The first part (PyTorch basics) is not assessed, but if you are not familiar with PyTorch or are unsure, we strongly recommend that you start with it. The second part (Practice) has graded and ungraded exercises on working with tensors and building your first neural network.\\\n",
    "The goal of the assignment is to gain practical skills in working with PyTorch.\n",
    "\n",
    "### Assessment and penalties\n",
    "Each of the tasks has a certain “cost” (indicated in brackets near the task). The maximum allowable grade for a task is 20 points.\n",
    "\n",
    "You may not turn in an assignment after a strict (Hard) deadline.\n",
    "\n",
    "The assignment is completed independently. “Similar” solutions are considered plagiarism and all students involved (including those who have been copied from) cannot receive more than 0 points for it. If you have found a solution to any of the assignments (or part of it) in an open source, you must provide a link to that source (most likely you will not be the only one who found it, so to exclude suspicion of plagiarism, a link to the source is required).\n",
    "\n",
    "---\n",
    "\n",
    "**When using AI, chatbots, generative and large language models** (ChatGPT, DeepSeek, Qwen, Llama, Mistral, Falcon, Gemma, Microsoft Copilot, Gemini, Claude, Grok, Perplexity, YandexGPT, GigaChat and others), **you must specify the following for each of them in LLM Documentation in the next cell**:\n",
    "- The full name and version of the model, as well as a link to the service used\n",
    "   * The application or browser version with built-in assistant, TG bot, etc.\n",
    "- **All** used prompts\n",
    "- Tell us how you rate the AI's work, and what specific problem did it help solve?\n",
    "- The verdict on recognizing the decision as written by AI without observing these rules is made by a teaching team.\n",
    "   * Such cases are to be considered as plagiarism.\n",
    "\n",
    "**CONSENT**.\n",
    "<input type=\"checkbox\" disabled checked /> I confirm that I will use AI agents in this home assignment only on the condition that they are documented.\n",
    "\n",
    "---\n",
    "\n",
    "### Format of submission\n",
    "Assignments are submitted through the Smart LMS system here https://edu.hse.ru/mod/assign/view.php?id=1944225. You should send a notebook with the completed assignment. Name the notebook itself in the format **HW1-pytorch-NameLastname.ipynb**, where Name and Lastname are your first and last name.\n",
    "\n",
    "For ease of checking yourself, calculate your maximum grade (based on the set of problems solved) and indicate it below.\n",
    "\n",
    "Score: **xx**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BaXlfHE_hLup"
   },
   "source": [
    "## 0. PyTorch basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnhT3CDa--Du"
   },
   "source": [
    "Inspired by https://nrehiew.github.io/blog/pytorch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wirAvJNxhLup"
   },
   "source": [
    "Load necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aQ2_j3nkhLuq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FA8a-cnn0gFC"
   },
   "source": [
    "PyTorch uses its own data representation (i.e. **tensor** objects) for several important reasons that are key to its purpose as an efficient, flexible, and scalable deep learning framework.\n",
    "\n",
    "PyTorch's custom data representation enables\n",
    "- High performance on hardware accelerators.\n",
    "- Support for automatic differentiation.\n",
    "- Flexibility through dynamic computational graphs.\n",
    "- Advanced multidimensional data manipulation.\n",
    "- Multi-device scalability.\n",
    "\n",
    "These features make PyTorch tensors ideal for deep learning, unlike traditional Python data structures such as lists or arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8RTaJJl8faQ"
   },
   "source": [
    "In the context of machine learning, a tensor is an *n*-dimensional matrix. OK, but what is a `torch.tensor`? More specifically, what actually happens when the following piece of code is executed: `a = torch.tensor(1.0, requires_grad=True)`? It turns out that PyTorch allocates the data on the heap and returns the pointer to that data as a shared pointer (see more [here](https://discuss.pytorch.org/t/where-does-torch-tensor-create-the-object-stack-or-heap/182753)). To better understand pointers in PyTorch, let's look at some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNtxji6Jgd2v"
   },
   "source": [
    "The following cell creates a tensor with shape `(2, 6)`, two rows and six columns, containing values randomly distributed according to a normal distribution with mean zero and standard deviation one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "r_ToDexwg0-X"
   },
   "outputs": [],
   "source": [
    "features = torch.randn((2, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s560vhGohKO8"
   },
   "source": [
    "The next cell creates another tensor with the same shape as the features, again containing values from a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1tAtv29HhMI9"
   },
   "outputs": [],
   "source": [
    "weights = torch.randn_like(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZutF30jhZih"
   },
   "source": [
    "PyTorch tensors can be added, multiplied, subtracted, etc., just like Numpy arrays. In general, you'll use PyTorch tensors pretty much the same way you use Numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZV7NQt8PmRFr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [6, 7, 8]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(9).reshape(3, 3) # 3x3 tensor\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0OfYkViyl-n"
   },
   "source": [
    "Original and derived tensors in PyTorch are objects linked to the same memory area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Wmb60ZeVypmi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[123,   1,   2],\n",
      "        [  3,   4,   5],\n",
      "        [  6,   7,   8]]) \n",
      "\n",
      " tensor([[123,   3,   6],\n",
      "        [  1,   4,   7],\n",
      "        [  2,   5,   8]])\n"
     ]
    }
   ],
   "source": [
    "b = a.t() # Transpose\n",
    "b[0, 0] = 123\n",
    "print(a, '\\n\\n', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gL_lk8o1zA2s"
   },
   "source": [
    "Note that `.t()` returns a pointer, which means that both a and b point to the same underlying data, and any changes to that underlying data can be seen from both pointers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWba5f6QzVHa"
   },
   "source": [
    "Compare with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "E89Sftf1mRQQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 42.0\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(9).reshape(3, 3) # torch.int64\n",
    "b = a.to(torch.float16) #\n",
    "b[0][0] = 42\n",
    "print(a[0][0].item(), b[0][0].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZlulCwVKhLur"
   },
   "source": [
    "PyTorch needs to represent a `float16` differently to how `int64` is represented in memory. So in this example PyTorch needs to make a copy of the data and represent it differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4wkVlIehLur"
   },
   "source": [
    "### PyTorch broadcasting\n",
    "\n",
    "If necessary, check the PyTorch [broadcasting semantics](https://pytorch.org/docs/stable/notes/broadcasting.html#general-semantics).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BIuaYSvqhLus"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1, 2]).reshape(1, 2) # 1 x 2\n",
    "b = torch.tensor([[3, 4], [5, 6]]) # 2 x 2\n",
    "c = torch.zeros((2, 2)) # we know that a + b gives a 2x2 tensor\n",
    "\n",
    "# Now working on: First dimension from the right\n",
    "c[0][0] = a[0][0] + b[0][0]\n",
    "c[0][1] = a[0][1] + b[0][1]\n",
    "\n",
    "# Now working on: Second dimension from the right\n",
    "# note that a has been broadcasted to 2x2 along the second dimension\n",
    "# we check that the broadcasted_shape[1] = 2 which is not equals to\n",
    "# the original shape[1] = 1 so we know this is a broadcasted dimension\n",
    "# Thus, we artifically just return the 0th element along this dimension\n",
    "c[1][0] = a[0][0] + b[1][0]\n",
    "c[1][1] = a[0][1] + b[1][1]\n",
    "\n",
    "torch.equal(a + b, c) # true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqHFyS9X_5TB"
   },
   "source": [
    "#### Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NgxuJauQ_8KQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 1, 3]) torch.Size([3, 4, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn((3, 4, 1, 2)) # 3 x 4 x 1 x 2\n",
    "b = torch.randn((1, 2, 3)) # 1 x 2 x 3\n",
    "\n",
    "# Matrix Multiply Shape: 1x2 @ 2x3 -> 1x3\n",
    "# Batch Shape: We broadcast (3, 4) and (1) -> (3, 4)\n",
    "# Result shape: 3 x 4 x 1 x 3\n",
    "c = torch.zeros((3, 4, 1, 3))\n",
    "\n",
    "# iterate over the batch dimensions of (3, 4)\n",
    "for i in range(3):\n",
    "\tfor j in range(4):\n",
    "\t\ta_slice = a[i][j] # 1 x 2\n",
    "\t\tb_slice = b[0] # 2 x 3\n",
    "\t\tc[i][j] = a_slice @ b_slice # 1 x 3\n",
    "\n",
    "print(torch.matmul(a, b).shape, c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWs6ljrIhLus"
   },
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGsDDU6rGlpe"
   },
   "source": [
    "The core of PyTorch is its automatic differentiation engine. In general terms, each time a differentiable operation between 2 tensors occurs, PyTorch will automatically build the entire computational graph through a callback function. The gradient of each tensor is then updated when `.backward()` is called. This is PyTorch's biggest abstraction. Often `.backward()` is called and we just hope that the gradients flow properly. In this section I will try to build some intuition for visualising gradient flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Hv0g-0B5Hqbg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "# Define a tensor with requires_grad=True to track computation\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# Define a function y = x^2\n",
    "y = x**2\n",
    "\n",
    "# Compute the gradient (dy/dx)\n",
    "y.backward()\n",
    "\n",
    "# x.grad will hold the derivative of y with respect to x\n",
    "print(x.grad)  # Output: tensor([4.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYhklXjXHree"
   },
   "source": [
    "A common problem in trying to understand backpropagation is that most people understand derivatives and the chain rule for scalars, but how this translates to higher dimensional tensors is not particularly obvious.\n",
    "\n",
    "Thinking about gradients from this local scalar perspective has the added benefit of making the effect of tensor operations on gradients intuitive. For example, operations such as `.reshape()`, `.transpose()`, `.cat()` and `.split()` do not affect the single value and its gradient on a local scale. It follows naturally that the effect of these operations on the gradient tensor of a tensor is the operation itself. For example, flattening a tensor with `.reshape(-1)` will have the same effect on its gradient as calling `.reshape(-1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0rwEkgRxIQEf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5., 14.],\n",
      "        [29., 50.]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)  # Shape: (2, 2)\n",
    "\n",
    "# Define a vector-valued function: f(z) = z^3 + 2z\n",
    "f = z**3 + 2 * z  # Element-wise\n",
    "\n",
    "# Compute gradients by summing over all outputs (scalarization)\n",
    "# The .backward() method requires scalar output, hence `sum()`\n",
    "f.sum().backward()\n",
    "\n",
    "# Gradients of f w.r.t the input z\n",
    "print(z.grad)  # Contains derivatives for each element in z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1JvoyxJIac7"
   },
   "source": [
    "PyTorch also supports higher-order derivatives (e.g., second or third derivatives). To compute these, you need to set `create_graph=True` for the first derivative computation. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Qs6xwnhuIdCg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First derivative: tensor([12.], grad_fn=<MulBackward0>)\n",
      "Second derivative: tensor([12.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# Define a function f(x) = x^3\n",
    "f = x**3\n",
    "\n",
    "# Compute first derivative df/dx using autograd.grad\n",
    "first_derivative = torch.autograd.grad(f, x, create_graph=True)[0]\n",
    "\n",
    "print(\"First derivative:\", first_derivative)\n",
    "\n",
    "# Compute second derivative d^2f/dx^2\n",
    "second_derivative = torch.autograd.grad(first_derivative, x)[0]\n",
    "\n",
    "print(\"Second derivative:\", second_derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFK1Mx2nJI3b"
   },
   "source": [
    "To compute the Jacobian (partial derivatives of each output w.r.t each input):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Sv9apb72JJtK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian matrix:\n",
      "tensor([[ 2.,  0.],\n",
      "        [ 0., 12.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "\n",
    "# Define a vector-valued function\n",
    "y = torch.stack([x[0]**2, x[1]**3])  # Outputs: [x[0]^2, x[1]^3]\n",
    "\n",
    "# Compute Jacobian (manually populate each row)\n",
    "jacobian = []\n",
    "for i in range(y.size(0)):\n",
    "    x.grad = None  # Clear previous gradients\n",
    "    y[i].backward(retain_graph=True)  # Compute partial derivatives\n",
    "    jacobian.append(x.grad.clone())  # Store the gradient for output i\n",
    "\n",
    "jacobian = torch.stack(jacobian)\n",
    "\n",
    "print(\"Jacobian matrix:\")\n",
    "print(jacobian)  # Displays the partial derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsQZKYEoFP5w"
   },
   "source": [
    "## 1. Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgeUaWR4D3xV"
   },
   "source": [
    "### Documentation\n",
    "Please use the PyTorch documentation in case of difficulties:\n",
    "* The documentation on [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html#torch-tensor)\n",
    "* The documentation on [`torch.cuda`](https://pytorch.org/docs/master/notes/cuda.html#cuda-semantics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vky1ZFTQFj4r"
   },
   "source": [
    "### [0 points] Create a random tensor with shape `(5, 5)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ZU1IDQwF40N"
   },
   "outputs": [],
   "source": [
    "# Create random tensor of given shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5swfTluGCGn"
   },
   "source": [
    "### [0 points] Perform matrix multiplication of the resulting tensor with another random tensor of shape `(1, 5)`.\n",
    "Hint: you may have to transpose the second tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4n_AwyjHGNv"
   },
   "outputs": [],
   "source": [
    "# Create another random tensor\n",
    "\n",
    "# Perform matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHVza-w5HymK"
   },
   "source": [
    "### [1 point] Set the random seed to `42` and repeat code in two previous cells.\n",
    "The output should be:\\\n",
    "```tensor([[1.1697],\n",
    "        [0.9278],\n",
    "        [0.9534],\n",
    "        [0.6976],\n",
    "        [0.7460]])```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7wGvpJ8mIH1Q"
   },
   "outputs": [],
   "source": [
    "# Set manual seed\n",
    "\n",
    "# Create two random tensors\n",
    "\n",
    "# Matrix multiply tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0j-wOBUJ7sZ"
   },
   "source": [
    "### [1 point] Set random seed on the GPU.\n",
    "\n",
    "Hint: You'll need to read the [`torch.cuda`](https://pytorch.org/docs/master/notes/cuda.html#cuda-semantics) documentation for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dL3xPknOKTqu"
   },
   "outputs": [],
   "source": [
    "# Set random seed on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiztDqPBMf92"
   },
   "source": [
    "### [1 point] Create two random tensors of shape `(3, 4)` and send them both to the GPU. Set random seed `123` when creating the tensors (this doesn't have to be the GPU random seed).\n",
    "\n",
    "Hint: The output should contain `device='cuda:0'`.\n",
    "Hint: How to access GPU in Colab?\n",
    "Setting up the Runtime: In Google Colab, go to the \"Runtime\" menu and select \"Change runtime type.\" A dialog box will appear where you can choose the runtime type and hardware accelerator. Select \"T4 GPU\" as the hardware accelerator and click \"Save.\" This step ensures that your Colab notebook is configured to use the GPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iphtzk8UN8Zq"
   },
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "\n",
    "# Check for access to GPU\n",
    "\n",
    "# Create two random tensors on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhy9lfR2OJn3"
   },
   "source": [
    "### [0 points] Perform a matrix multiplication on the tensors you created in previous cell.\n",
    "Hint: you may have to adjust the shapes of one of the tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOLT8xgaOsV3"
   },
   "outputs": [],
   "source": [
    "# Perform matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYNmDRMJPC0n"
   },
   "source": [
    "### [0 points] Find the minimum and maximum values of the output of previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbK9-0ekPN0_"
   },
   "outputs": [],
   "source": [
    "# Find min\n",
    "\n",
    "# Find max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zqi0bF0DPWq7"
   },
   "source": [
    "### [0 points] Find the indices of these minimum and maximum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrFKBNZ6Pj0_"
   },
   "outputs": [],
   "source": [
    "# Find arg min\n",
    "\n",
    "# Find arg max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58g_O7LAQcxT"
   },
   "source": [
    "### [2 points] Make a random tensor with shape `(1, 1, 1, 8)` and then create a new tensor with all the `1` dimensions removed to be left with a tensor of shape `(8)`. Set the seed to `999` when you create it and print out the first tensor and it's shape as well as the second tensor and it's shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XG3kPy8oQ004"
   },
   "outputs": [],
   "source": [
    "# Set seed\n",
    "\n",
    "# Create random tensor\n",
    "\n",
    "# Remove single dimensions\n",
    "\n",
    "# Print out tensors and their shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSTu45lMRkwT"
   },
   "source": [
    "### [1 point] Print the index of the maximum value of the second tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ZyDX9yVRv5j"
   },
   "outputs": [],
   "source": [
    "# Find arg max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKI_cFegJZng"
   },
   "source": [
    "### Your first neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dskygzU9N9JG"
   },
   "source": [
    "In this exercise you will implement a small neural network with two linear layers. The first layer takes an eight-dimensional input and the last layer outputs a one-dimensional tensor.\n",
    "The following exercices are inspired by the DataCamp course [Introduction to Deep Learning with PyTorch](https://app.datacamp.com/learn/courses/introduction-to-deep-learning-with-pytorch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSiXrpUvN_zi"
   },
   "source": [
    "### [0 points] Create a neural network of **two linear layers** that takes `input_tensor` as input, representing `8` features, and outputs a tensor of dimensions `1`.\n",
    "Hint: use [`torch.nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) and [`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVFHvZHlNjOF"
   },
   "outputs": [],
   "source": [
    "input_tensor = torch.Tensor([[1, 5, 4, 7, 3, 6, 0, 2]])\n",
    "\n",
    "# Implement a small neural network with two linear layers\n",
    "model = nn.Sequential(\n",
    "  # write your code here\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2UPUEiTTyNJ"
   },
   "source": [
    "### [0 points] Create a [`torch.nn.Sigmoid`](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html) and apply it on `input_tensor` to generate a probability for a binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sqLhiYvCT3t8"
   },
   "outputs": [],
   "source": [
    "input_tensor = torch.tensor([[1.3]])\n",
    "\n",
    "# Create a sigmoid function and apply it on input_tensor\n",
    "sigmoid = # write your code here\n",
    "probability = sigmoid(input_tensor)\n",
    "print(probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrMRn6INVgMF"
   },
   "source": [
    "### [0 points] Create a [`torch.nn.Softmax`](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) and apply it on `input_tensor` to generate a probability for a multiclass classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9kAiY3hTSsi"
   },
   "outputs": [],
   "source": [
    "input_tensor = torch.tensor([[6.6, -3.2, -4.3, 0.3, -0.7, -4.7]])\n",
    "\n",
    "# Create a softmax function and apply it on input_tensor\n",
    "softmax = # write your code here\n",
    "probabilities = softmax(input_tensor)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMsMLm0QWJah"
   },
   "source": [
    "### [0 points] How can you avoid the following warning message when calling the softmax function from PyTorch?\n",
    "\n",
    "> UserWarning: Implicit dimension choice for softmax has been deprecated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijdpDwEn6i03"
   },
   "source": [
    "### Neural Networks for Classification and Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvsqtcVVwHwh"
   },
   "source": [
    "### [2 points] Create a neural network that takes a tensor of dimension `1x8` as input and returns an output of the correct shape for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXIZQDMkwS8l"
   },
   "outputs": [],
   "source": [
    "input_tensor = torch.Tensor([[1, 5, 4, 7, 3, 6, 0, 2]])\n",
    "\n",
    "# Implement a small neural network for binary classification\n",
    "# Pass the output of the linear layer to a sigmoid, which both takes in and return a single float.\n",
    "model = nn.Sequential(\n",
    "# write your code here\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYg4Pt2EyYVe"
   },
   "source": [
    "### [2 points] Create a 4-layer linear neural network compatible with input_tensor as input and a regression value as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bMN4ndYnyvtz"
   },
   "outputs": [],
   "source": [
    "input_tensor = torch.Tensor([[1, 5, 4, 7, 3, 6, 0, 2, 6, 2]])\n",
    "\n",
    "# Implement a neural network with exactly four linear layers\n",
    "model = nn.Sequential(\n",
    "# write your code here\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alY7CVjYzOhX"
   },
   "source": [
    "### [1 point] Create a one-hot encoded vector of the ground truth label `y` using [`torch.nn.functional.one_hot`](https://pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4m1a5bQZzVhl"
   },
   "outputs": [],
   "source": [
    "y = 1\n",
    "num_classes = 4\n",
    "\n",
    "# Create the one-hot encoded vector using PyTorch\n",
    "one_hot = # write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kz3qoYAAz2qt"
   },
   "source": [
    "### [2 points] Calculate the cross entropy loss using the one-hot encoded vector of the ground truth label `y`, with `4` features (one for each class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f00cdahH0EF_"
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "y = [2]\n",
    "scores = torch.tensor([[3.1, -5.0, 1.8, 4.3]])\n",
    "\n",
    "# Create a one-hot encoded vector of the label y\n",
    "one_hot_label = # write your code here\n",
    "\n",
    "# Create the cross entropy loss function\n",
    "criterion = # write your code here\n",
    "\n",
    "# Calculate the cross entropy loss\n",
    "loss = # write your code here\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kF3LyK8K1moJ"
   },
   "source": [
    "### [1 point] Accessing the model parameters.\n",
    "Hint: model parameters are weights and biases.\\\n",
    "Hint: use [`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) documentation.\\\n",
    "Hint: try to discover [PyTorch discussion forum](https://discuss.pytorch.org/t/access-weights-of-a-specific-module-in-nn-sequential/3627)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8oF_In8b3q0m"
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(16, 8),\n",
    "                      nn.Linear(8, 4)\n",
    "                     )\n",
    "\n",
    "# Access the weight of the first linear layer\n",
    "weight_0 = # write your code here\n",
    "\n",
    "# Access the bias of the second linear layer\n",
    "bias_1 = # write your code here\n",
    "\n",
    "print(weight_0, bias_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkVcOpLJ4QLN"
   },
   "source": [
    "### [3 point] Create a 3-layer neural network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kt4_rCUu_KAP"
   },
   "outputs": [],
   "source": [
    "input_tensor = torch.Tensor([[1, 5, 4, 7, 3, 6, 0, 2, 6, 2, 4, 1, 4, 0, 5, 12]])\n",
    "\n",
    "# Create the model of three linear layers and Softmax activation\n",
    "model = nn.Sequential(\n",
    "# write your code here\n",
    ")\n",
    "\n",
    "# Run a forward pass\n",
    "prediction = model(input_tensor)\n",
    "\n",
    "# Calculate the loss\n",
    "criterion = CrossEntropyLoss()\n",
    "target = tensor([[1., 0.]])\n",
    "\n",
    "loss = criterion(prediction, target)\n",
    "\n",
    "# Compute the gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aagrXWMeDmzp"
   },
   "source": [
    "### [2 points] Update the weights of the first layer using the gradients scaled by the learning rate `0.001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qpWTEfGx4VdE"
   },
   "outputs": [],
   "source": [
    "# Learning rate is typically small\n",
    "lr = 0.001\n",
    "\n",
    "weight = # write your code here\n",
    "\n",
    "# Access the gradients of the weight of each linear layer\n",
    "grads = # write your code here\n",
    "\n",
    "# Update the weights using the learning rate and the gradients\n",
    "weight = # write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vju07zzBFL4p"
   },
   "source": [
    "### [1 point] Update the model's parameters using the [`torch.optim.SGD`](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) optimizer and the learning rate `0.001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RmnIOzBSFabG"
   },
   "outputs": [],
   "source": [
    "# Create the SGD optimizer\n",
    "optimizer = # write your code here\n",
    "\n",
    "loss = criterion(prediction, target)\n",
    "loss.backward()\n",
    "\n",
    "# Update the model's parameters using the optimizer\n",
    "\n",
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xQjV9BuhLu6"
   },
   "source": [
    "That's all for this PyTorch practice.\n",
    "\n",
    "**Don't forget to rename the jupyter notebook to HW1-pytorch-NameLastname.ipynb, where Name and Lastname are your first and last name. Then send a jupyter notebook file to SmartLMS.**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
